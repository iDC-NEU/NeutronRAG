'''
Author: lpz 1565561624@qq.com
Date: 2025-07-29 14:26:11
LastEditors: lpz 1565561624@qq.com
LastEditTime: 2025-07-30 16:09:04
FilePath: /lipz/NeutronRAG/NeutronRAG/backend/schedule/schedular.py
Description: è¿™æ˜¯é»˜è®¤è®¾ç½®,è¯·è®¾ç½®`customMade`, æ‰“å¼€koroFileHeaderæŸ¥çœ‹é…ç½® è¿›è¡Œè®¾ç½®: https://github.com/OBKoro1/koro1FileHeader/wiki/%E9%85%8D%E7%BD%AE
'''
import time
from collections import deque
from typing import Deque, List, Optional
from llmragenv.Cons_Retri.KG_Retriever import RetrieverGraph,RetrieverEntities
from concurrent.futures import ThreadPoolExecutor, as_completed
from database.vector.Milvus.milvus import MilvusDB, myMilvus
from llmragenv.demo_chat import Demo_chat
from schedule.request import *


WAITING = "waiting"
PROCESSING = "processing"
FINISHED = "finished"


class Schedular:
    def __init__(self):
        self.waiting_queue: Deque[Request] = deque()        # ç­‰å¾…å¤„ç†çš„è¯·æ±‚
        self.processing_queue: Deque[Request] = deque()     # æ­£åœ¨å¤„ç†çš„è¯·æ±‚
        self.finished: List[Request] = []                   # å®Œæˆçš„è¯·æ±‚ï¼Œå‡†å¤‡è¿”å›ç»™ç”¨æˆ·
    def add_request(self, request):
        """æ·»åŠ æ–°è¯·æ±‚"""
        self.waiting_queue.append(request)
        print(f"[{time.strftime('%H:%M:%S')}] New request queued: {request.user_id} â†’ {request.query}")


    def dispatch_to_processing(self):
        """ä» waiting_queue åˆ†å‘åˆ° processing_queue"""
        if self.waiting_queue:
            req = self.waiting_queue.popleft()
            self.processing_queue.append(req)
            req.state = PROCESSING
            print(f"Dispatching to retrieval: {req.user_id} â†’ {req.query}")
            return req
        return None
    
    def finish_retrieval(self, req:Request):
        """æ£€ç´¢å¤„ç†å®Œæˆï¼Œç§»åŠ¨åˆ° llm_queue"""
        if req in self.processing_queue:
            self.processing_queue.remove(req)
        req.state = "retrieved"
        self.finished.append(req)
        print(f"â†’ Retrieval complete for: {req.user_id} â†’ {req.query}")

    def dispatch_to_llm(self):
        """ä» llm_queue ä¸­å–å‡ºå‡†å¤‡äº¤ç»™ LLM"""
        if self.finished:
            req = self.finished.popleft()
            req.state = "generating"
            print(f"Sending to LLM: {req.user_id} â†’ {req.query}")
            return req
        return None
    
    def finish_request(self, req, llm_output=None):
        """LLM ç”Ÿæˆå®Œæˆï¼Œè®°å½•ç»“æœï¼Œæ ‡è®°ä¸ºå®Œæˆ"""
        req.state = "finished"
        req.response = llm_output
        self.finished.append(req)
        print(f"âœ… Request finished: {req.user_id} â†’ {req.query}")


    def handle_batch(self,batch_size=4):
        if not self.waiting_queue:
            print("âš ï¸ No request in waiting_queue.")
            return
        
        batch = []
        for _ in range(min(batch_size, len(self.waiting_queue))):
            req = self.waiting_queue.popleft()
            req.state = PendingDeprecationWarning
            self.processing_queue.append(req)
            batch.append(req)


        def process(req):
            try:
                # åŸºäº Request æ„é€  Demo_chat å®ä¾‹
                current_model = Demo_chat.from_request(req)
                # å¤„ç†è¯·æ±‚ï¼ˆå®Œæˆå‘é‡+å›¾æ£€ç´¢ï¼‰
                current_model.handle_one_request(req=req)
                self.finished.append(req)
                return req, True
            except Exception as e:
                print(f"âŒ Retrieval failed for {req.user_id}: {e}")
                return req, False

        with ThreadPoolExecutor(max_workers=batch_size) as executor:
            futures = [executor.submit(process, req) for req in batch]

            for future in as_completed(futures):
                req, success = future.result()
                self.processing_queue.remove(req)
                if success:
                    self.finished.append(req)
                    print(f"âœ… Retrieval finished: {req.user_id} â†’ {req.query}")
                else:
                    req.state = "error"
                    print(f"ğŸš« Marked as error: {req.user_id}")